{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "047f6e79",
   "metadata": {},
   "source": [
    "# Master ETL Pipeline for Job Data Processing\n",
    "\n",
    "**Enterprise-Ready Data Cleaning Pipeline**\n",
    "\n",
    "This notebook provides a comprehensive, user-friendly interface for processing multiple job data CSV files into a single, clean master dataset.\n",
    "\n",
    "## Features\n",
    "- \ud83d\udd04 **Multi-file processing** - Upload and process multiple CSV files simultaneously\n",
    "- \ud83d\udcca **Schema validation** - Automatic detection and handling of schema inconsistencies  \n",
    "- \ud83c\udfaf **Master file output** - Consolidated dataset with job_id as primary key\n",
    "- \ud83d\udcc8 **Progress tracking** - Real-time processing updates with visual feedback\n",
    "- \u2601\ufe0f **BigQuery integration** - Optional direct upload to Google BigQuery\n",
    "- \ud83d\udccb **Comprehensive reporting** - Detailed data quality and transformation reports\n",
    "\n",
    "## Instructions\n",
    "**For non-technical users:** Simply run cells 1-6 in order. Upload your CSV files when prompted, configure output options, and download your cleaned master dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17729ec",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Validation\n",
    "\n",
    "Setting up the processing environment and validating all required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7648fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import logging\n",
    "from functools import reduce\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Interactive widgets and display\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output, Markdown\n",
    "\n",
    "# File handling\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Optional BigQuery support\n",
    "try:\n",
    "    from google.cloud import bigquery\n",
    "    BIGQUERY_AVAILABLE = True\n",
    "    print(\"\u2705 BigQuery integration available\")\n",
    "except ImportError:\n",
    "    BIGQUERY_AVAILABLE = False\n",
    "    print(\"\u2139\ufe0f BigQuery not available (install google-cloud-bigquery for BigQuery features)\")\n",
    "\n",
    "# Import ETL logic\n",
    "try:\n",
    "    from etl_pipeline_logic import assess_raw_data, clean_csv_data, upload_to_bigquery, validate_bigquery_config\n",
    "    ETL_LOGIC_AVAILABLE = True\n",
    "    print(\"\u2705 ETL processing logic loaded successfully\")\n",
    "except ImportError:\n",
    "    ETL_LOGIC_AVAILABLE = False\n",
    "    print(\"\u274c ETL logic module not found. Ensure 'etl_pipeline_logic.py' is in the same directory.\")\n",
    "\n",
    "# Validate core packages\n",
    "required_packages = {'pandas': pd, 'numpy': np, 'ipywidgets': widgets}\n",
    "missing_packages = []\n",
    "\n",
    "for package_name, package_module in required_packages.items():\n",
    "    try:\n",
    "        version = package_module.__version__\n",
    "        print(f\"\u2705 {package_name}: {version}\")\n",
    "    except AttributeError:\n",
    "        print(f\"\u2705 {package_name}: Available\")\n",
    "\n",
    "if missing_packages:\n",
    "    print(f\"\\n\u274c Missing packages: {', '.join(missing_packages)}\")\n",
    "    print(\"Please install missing packages before proceeding.\")\n",
    "    ENVIRONMENT_READY = False\n",
    "else:\n",
    "    ENVIRONMENT_READY = True\n",
    "    print(f\"\\n\ud83d\ude80 Environment ready for processing! All systems operational.\")\n",
    "\n",
    "# Configure logging for this session\n",
    "log_filename = f\"etl_session_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_filename, mode='w'),\n",
    "        logging.StreamHandler()\n",
    "    ],\n",
    "    force=True\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "print(f\"\ud83d\udcdd Session log: {log_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b3ba79",
   "metadata": {},
   "source": [
    "## 2. File Upload & Configuration\n",
    "\n",
    "Upload your CSV files and configure processing options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cdfdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ENVIRONMENT_READY or not ETL_LOGIC_AVAILABLE:\n",
    "    display(HTML('<div style=\"background-color: #ffebee; padding: 15px; border-radius: 5px; border-left: 4px solid #f44336;\"><strong>\u26a0\ufe0f Cannot proceed:</strong> Environment setup failed or ETL logic unavailable. Please resolve issues above.</div>'))\n",
    "else:\n",
    "    # File upload widget\n",
    "    file_uploader = widgets.FileUpload(\n",
    "        accept='.csv',\n",
    "        multiple=True,\n",
    "        description='Upload CSV Files',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(width='100%')\n",
    "    )\n",
    "    \n",
    "    # Output configuration\n",
    "    output_name = widgets.Text(\n",
    "        value='master_job_data',\n",
    "        description='Master file name:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(width='300px')\n",
    "    )\n",
    "    \n",
    "    # BigQuery configuration (if available)\n",
    "    if BIGQUERY_AVAILABLE:\n",
    "        enable_bigquery = widgets.Checkbox(\n",
    "            value=False,\n",
    "            description='Upload to BigQuery',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        bq_project = widgets.Text(\n",
    "            value='',\n",
    "            description='GCP Project ID:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='300px')\n",
    "        )\n",
    "        \n",
    "        bq_dataset = widgets.Text(\n",
    "            value='job_data',\n",
    "            description='Dataset Name:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='300px')\n",
    "        )\n",
    "        \n",
    "        bq_table = widgets.Text(\n",
    "            value='cleaned_jobs',\n",
    "            description='Table Name:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='300px')\n",
    "        )\n",
    "    \n",
    "    # Processing options\n",
    "    handle_schema_mismatch = widgets.Dropdown(\n",
    "        options=['Auto-align', 'Strict validation', 'Skip mismatched'],\n",
    "        value='Auto-align',\n",
    "        description='Schema handling:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    merge_strategy = widgets.Dropdown(\n",
    "        options=['Outer join (keep all records)', 'Inner join (matching records only)', 'No merge (separate files)'],\n",
    "        value='Outer join (keep all records)',\n",
    "        description='Merge strategy:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    # Display configuration UI\n",
    "    display(HTML('<h3>\ud83d\udcc1 File Upload</h3>'))\n",
    "    display(file_uploader)\n",
    "    \n",
    "    display(HTML('<h3>\u2699\ufe0f Configuration</h3>'))\n",
    "    config_box = widgets.VBox([\n",
    "        widgets.HBox([\n",
    "            widgets.VBox([output_name, handle_schema_mismatch]),\n",
    "            widgets.VBox([merge_strategy])\n",
    "        ])\n",
    "    ])\n",
    "    display(config_box)\n",
    "    \n",
    "    if BIGQUERY_AVAILABLE:\n",
    "        display(HTML('<h3>\u2601\ufe0f BigQuery Options (Optional)</h3>'))\n",
    "        bq_box = widgets.VBox([\n",
    "            enable_bigquery,\n",
    "            widgets.HBox([bq_project, bq_dataset, bq_table])\n",
    "        ])\n",
    "        display(bq_box)\n",
    "    \n",
    "    # Status display area\n",
    "    status_output = widgets.Output()\n",
    "    display(HTML('<h3>\ud83d\udcca Processing Status</h3>'))\n",
    "    display(status_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b607062",
   "metadata": {},
   "source": [
    "## 3. Schema Analysis & Validation\n",
    "\n",
    "Analyzing uploaded files for schema consistency and data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac7afd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_file_schemas(uploaded_files):\n",
    "    \"\"\"Analyze schemas of all uploaded files\"\"\"\n",
    "    schemas = {}\n",
    "    file_info = {}\n",
    "    \n",
    "    for file_name, file_info_dict in uploaded_files.items():\n",
    "        try:\n",
    "            content = file_info_dict['content']\n",
    "            \n",
    "            # Quick assessment\n",
    "            assessment = assess_raw_data(content, file_name)\n",
    "            \n",
    "            # Load first few rows to get schema\n",
    "            df_sample = pd.read_csv(\n",
    "                io.BytesIO(content),\n",
    "                delimiter=assessment.get('likely_delimiter', ','),\n",
    "                encoding=assessment.get('working_encoding', 'utf-8'),\n",
    "                nrows=5\n",
    "            )\n",
    "            \n",
    "            schemas[file_name] = {\n",
    "                'columns': list(df_sample.columns),\n",
    "                'dtypes': df_sample.dtypes.to_dict(),\n",
    "                'row_count_sample': len(df_sample),\n",
    "                'assessment': assessment\n",
    "            }\n",
    "            \n",
    "            file_info[file_name] = {\n",
    "                'size_mb': len(content) / (1024 * 1024),\n",
    "                'encoding': assessment.get('working_encoding', 'utf-8'),\n",
    "                'delimiter': assessment.get('likely_delimiter', ','),\n",
    "                'issues': assessment.get('issues_found', [])\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            schemas[file_name] = {'error': str(e)}\n",
    "            file_info[file_name] = {'error': str(e)}\n",
    "    \n",
    "    return schemas, file_info\n",
    "\n",
    "def check_schema_compatibility(schemas):\n",
    "    \"\"\"Check if schemas are compatible for merging\"\"\"\n",
    "    if len(schemas) <= 1:\n",
    "        return True, []\n",
    "    \n",
    "    # Get all unique columns across files\n",
    "    all_columns = set()\n",
    "    for schema in schemas.values():\n",
    "        if 'columns' in schema:\n",
    "            all_columns.update(schema['columns'])\n",
    "    \n",
    "    # Check for common job_id extraction potential\n",
    "    has_url_column = {}\n",
    "    for file_name, schema in schemas.items():\n",
    "        if 'columns' in schema:\n",
    "            url_cols = [col for col in schema['columns'] if any(keyword in col.lower() for keyword in ['url', 'link', 'href'])]\n",
    "            has_url_column[file_name] = len(url_cols) > 0\n",
    "    \n",
    "    compatibility_issues = []\n",
    "    \n",
    "    # Check for major column differences\n",
    "    column_sets = {}\n",
    "    for file_name, schema in schemas.items():\n",
    "        if 'columns' in schema:\n",
    "            column_sets[file_name] = set(schema['columns'])\n",
    "    \n",
    "    if len(column_sets) > 1:\n",
    "        common_columns = set.intersection(*column_sets.values())\n",
    "        if len(common_columns) < 3:  # Very few common columns\n",
    "            compatibility_issues.append(f\"Files have very few common columns ({len(common_columns)})\")\n",
    "    \n",
    "    return len(compatibility_issues) == 0, compatibility_issues\n",
    "\n",
    "# Schema analysis button and results\n",
    "analyze_button = widgets.Button(\n",
    "    description='\ud83d\udd0d Analyze Schemas',\n",
    "    button_style='info',\n",
    "    layout=widgets.Layout(width='200px')\n",
    ")\n",
    "\n",
    "schema_output = widgets.Output()\n",
    "\n",
    "def on_analyze_click(b):\n",
    "    with schema_output:\n",
    "        clear_output()\n",
    "        \n",
    "        if not file_uploader.value:\n",
    "            display(HTML('<div style=\"background-color: #fff3e0; padding: 10px; border-radius: 5px;\"><strong>\u2139\ufe0f No files uploaded yet.</strong> Please upload CSV files first.</div>'))\n",
    "            return\n",
    "        \n",
    "        display(HTML('<div style=\"background-color: #e8f5e8; padding: 10px; border-radius: 5px;\"><strong>\ud83d\udd0d Analyzing schemas...</strong></div>'))\n",
    "        \n",
    "        try:\n",
    "            schemas, file_info = analyze_file_schemas(file_uploader.value)\n",
    "            compatible, issues = check_schema_compatibility(schemas)\n",
    "            \n",
    "            # Display file information table\n",
    "            display(HTML('<h4>\ud83d\udccb File Information</h4>'))\n",
    "            \n",
    "            file_data = []\n",
    "            for file_name in schemas.keys():\n",
    "                info = file_info.get(file_name, {})\n",
    "                schema = schemas.get(file_name, {})\n",
    "                \n",
    "                if 'error' in schema:\n",
    "                    status = f\"\u274c Error: {schema['error']}\"\n",
    "                    columns = \"N/A\"\n",
    "                    size = \"N/A\"\n",
    "                else:\n",
    "                    status = \"\u2705 Valid\" if not info.get('issues') else f\"\u26a0\ufe0f {len(info['issues'])} issues\"\n",
    "                    columns = f\"{len(schema['columns'])} columns\"\n",
    "                    size = f\"{info.get('size_mb', 0):.2f} MB\"\n",
    "                \n",
    "                file_data.append([file_name, status, columns, size])\n",
    "            \n",
    "            file_df = pd.DataFrame(file_data, columns=['File Name', 'Status', 'Columns', 'Size'])\n",
    "            display(file_df)\n",
    "            \n",
    "            # Display compatibility status\n",
    "            if compatible:\n",
    "                display(HTML('<div style=\"background-color: #e8f5e8; padding: 10px; border-radius: 5px; margin-top: 10px;\"><strong>\u2705 Schema Compatibility:</strong> Files are compatible for merging</div>'))\n",
    "            else:\n",
    "                display(HTML(f'<div style=\"background-color: #ffebee; padding: 10px; border-radius: 5px; margin-top: 10px;\"><strong>\u26a0\ufe0f Schema Issues:</strong><br>{\"<br>\".join(issues)}</div>'))\n",
    "            \n",
    "            # Store analysis results for next step\n",
    "            global schema_analysis_results\n",
    "            schema_analysis_results = {\n",
    "                'schemas': schemas,\n",
    "                'file_info': file_info,\n",
    "                'compatible': compatible,\n",
    "                'issues': issues\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            display(HTML(f'<div style=\"background-color: #ffebee; padding: 10px; border-radius: 5px;\"><strong>\u274c Analysis Error:</strong> {str(e)}</div>'))\n",
    "\n",
    "analyze_button.on_click(on_analyze_click)\n",
    "\n",
    "if ENVIRONMENT_READY and ETL_LOGIC_AVAILABLE:\n",
    "    display(analyze_button)\n",
    "    display(schema_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047baddc",
   "metadata": {},
   "source": [
    "## 4. Data Processing & Cleaning\n",
    "\n",
    "Processing all uploaded files with real-time progress tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ab1b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files_with_progress(uploaded_files, config):\n",
    "    \"\"\"Process all files with progress tracking\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Create progress widgets\n",
    "    overall_progress = widgets.IntProgress(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=len(uploaded_files),\n",
    "        description='Overall:',\n",
    "        bar_style='info',\n",
    "        style={'bar_color': '#2196F3'},\n",
    "        layout=widgets.Layout(width='100%')\n",
    "    )\n",
    "    \n",
    "    current_file_label = widgets.HTML(value=\"<b>Ready to start processing...</b>\")\n",
    "    file_progress = widgets.IntProgress(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=100,\n",
    "        description='Current file:',\n",
    "        bar_style='info',\n",
    "        style={'bar_color': '#4CAF50'},\n",
    "        layout=widgets.Layout(width='100%')\n",
    "    )\n",
    "    \n",
    "    progress_display = widgets.VBox([\n",
    "        widgets.HTML(\"<h4>\ud83d\udcc8 Processing Progress</h4>\"),\n",
    "        overall_progress,\n",
    "        current_file_label,\n",
    "        file_progress\n",
    "    ])\n",
    "    \n",
    "    display(progress_display)\n",
    "    \n",
    "    # Process each file\n",
    "    for i, (file_name, file_info_dict) in enumerate(uploaded_files.items()):\n",
    "        current_file_label.value = f\"<b>Processing: {file_name}</b>\"\n",
    "        file_progress.value = 0\n",
    "        \n",
    "        try:\n",
    "            content = file_info_dict['content']\n",
    "            \n",
    "            # Step 1: Assessment (20%)\n",
    "            file_progress.value = 20\n",
    "            assessment = assess_raw_data(content, file_name)\n",
    "            \n",
    "            # Step 2: Cleaning (80%)\n",
    "            file_progress.value = 50\n",
    "            cleaned_df, report_df = clean_csv_data(content, file_name, assessment)\n",
    "            file_progress.value = 100\n",
    "            \n",
    "            if cleaned_df is not None:\n",
    "                # Add source file column for traceability\n",
    "                cleaned_df['source_file'] = file_name\n",
    "                \n",
    "                results[file_name] = {\n",
    "                    'success': True,\n",
    "                    'dataframe': cleaned_df,\n",
    "                    'report': report_df,\n",
    "                    'assessment': assessment,\n",
    "                    'original_rows': len(cleaned_df),\n",
    "                    'job_ids_found': cleaned_df['job_id'].notna().sum() if 'job_id' in cleaned_df.columns else 0\n",
    "                }\n",
    "            else:\n",
    "                results[file_name] = {\n",
    "                    'success': False,\n",
    "                    'error': 'Failed to process file',\n",
    "                    'assessment': assessment\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            results[file_name] = {\n",
    "                'success': False,\n",
    "                'error': str(e)\n",
    "            }\n",
    "        \n",
    "        # Update overall progress\n",
    "        overall_progress.value = i + 1\n",
    "    \n",
    "    current_file_label.value = f\"<b>\u2705 Processing complete! Processed {len(uploaded_files)} files.</b>\"\n",
    "    overall_progress.bar_style = 'success'\n",
    "    file_progress.bar_style = 'success'\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Processing button and controls\n",
    "process_button = widgets.Button(\n",
    "    description='\ud83d\ude80 Process All Files',\n",
    "    button_style='success',\n",
    "    layout=widgets.Layout(width='200px')\n",
    ")\n",
    "\n",
    "processing_output = widgets.Output()\n",
    "\n",
    "def on_process_click(b):\n",
    "    with processing_output:\n",
    "        clear_output()\n",
    "        \n",
    "        if not file_uploader.value:\n",
    "            display(HTML('<div style=\"background-color: #fff3e0; padding: 10px; border-radius: 5px;\"><strong>\u2139\ufe0f No files to process.</strong> Please upload CSV files first.</div>'))\n",
    "            return\n",
    "        \n",
    "        # Get configuration\n",
    "        config = {\n",
    "            'output_name': output_name.value,\n",
    "            'schema_handling': handle_schema_mismatch.value,\n",
    "            'merge_strategy': merge_strategy.value\n",
    "        }\n",
    "        \n",
    "        if BIGQUERY_AVAILABLE and 'enable_bigquery' in locals():\n",
    "            config['bigquery'] = {\n",
    "                'enabled': enable_bigquery.value,\n",
    "                'project': bq_project.value,\n",
    "                'dataset': bq_dataset.value,\n",
    "                'table': bq_table.value\n",
    "            }\n",
    "        \n",
    "        # Start processing\n",
    "        logger.info(f\"Starting batch processing of {len(file_uploader.value)} files\")\n",
    "        \n",
    "        global processing_results\n",
    "        processing_results = process_files_with_progress(file_uploader.value, config)\n",
    "        \n",
    "        # Display summary\n",
    "        successful_files = [name for name, result in processing_results.items() if result.get('success', False)]\n",
    "        failed_files = [name for name, result in processing_results.items() if not result.get('success', False)]\n",
    "        \n",
    "        display(HTML('<h4>\ud83d\udcca Processing Summary</h4>'))\n",
    "        \n",
    "        if successful_files:\n",
    "            total_rows = sum(processing_results[name]['original_rows'] for name in successful_files)\n",
    "            total_job_ids = sum(processing_results[name]['job_ids_found'] for name in successful_files)\n",
    "            \n",
    "            display(HTML(f'''\n",
    "            <div style=\"background-color: #e8f5e8; padding: 15px; border-radius: 5px; margin: 10px 0;\">\n",
    "                <strong>\u2705 Successfully processed {len(successful_files)} files:</strong><br>\n",
    "                \ud83d\udcca Total rows: {total_rows:,}<br>\n",
    "                \ud83c\udd94 Job IDs extracted: {total_job_ids:,}<br>\n",
    "                \ud83d\udcc1 Files: {\", \".join(successful_files)}\n",
    "            </div>\n",
    "            '''))\n",
    "        \n",
    "        if failed_files:\n",
    "            display(HTML(f'''\n",
    "            <div style=\"background-color: #ffebee; padding: 15px; border-radius: 5px; margin: 10px 0;\">\n",
    "                <strong>\u274c Failed to process {len(failed_files)} files:</strong><br>\n",
    "                \ud83d\udcc1 Files: {\", \".join(failed_files)}\n",
    "            </div>\n",
    "            '''))\n",
    "            \n",
    "            for file_name in failed_files:\n",
    "                error_msg = processing_results[file_name].get('error', 'Unknown error')\n",
    "                display(HTML(f'<div style=\"margin-left: 20px; color: #d32f2f;\"><strong>{file_name}:</strong> {error_msg}</div>'))\n",
    "\n",
    "process_button.on_click(on_process_click)\n",
    "\n",
    "if ENVIRONMENT_READY and ETL_LOGIC_AVAILABLE:\n",
    "    display(process_button)\n",
    "    display(processing_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4171355e",
   "metadata": {},
   "source": [
    "## 5. Master File Creation & Merging\n",
    "\n",
    "Creating the consolidated master dataset from all processed files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02bb34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_master_dataset(processing_results, merge_strategy):\n",
    "    \"\"\"Create master dataset from processing results\"\"\"\n",
    "    \n",
    "    # Get successful dataframes\n",
    "    successful_dfs = []\n",
    "    for file_name, result in processing_results.items():\n",
    "        if result.get('success') and 'dataframe' in result:\n",
    "            df = result['dataframe']\n",
    "            if df is not None and not df.empty:\n",
    "                successful_dfs.append(df)\n",
    "    \n",
    "    if not successful_dfs:\n",
    "        return None, \"No valid dataframes to merge\"\n",
    "    \n",
    "    if len(successful_dfs) == 1:\n",
    "        return successful_dfs[0], \"Single file - no merging needed\"\n",
    "    \n",
    "    try:\n",
    "        if merge_strategy == 'Outer join (keep all records)':\n",
    "            # Merge all dataframes on job_id with outer join\n",
    "            master_df = successful_dfs[0]\n",
    "            for df in successful_dfs[1:]:\n",
    "                master_df = pd.merge(master_df, df, on='job_id', how='outer', suffixes=('', '_dup'))\n",
    "                \n",
    "                # Remove duplicate columns (keep the first occurrence)\n",
    "                duplicate_cols = [col for col in master_df.columns if col.endswith('_dup')]\n",
    "                master_df = master_df.drop(columns=duplicate_cols)\n",
    "            \n",
    "            merge_info = f\"Outer join merge of {len(successful_dfs)} files\"\n",
    "            \n",
    "        elif merge_strategy == 'Inner join (matching records only)':\n",
    "            # Merge all dataframes on job_id with inner join\n",
    "            master_df = successful_dfs[0]\n",
    "            for df in successful_dfs[1:]:\n",
    "                master_df = pd.merge(master_df, df, on='job_id', how='inner', suffixes=('', '_dup'))\n",
    "                \n",
    "                # Remove duplicate columns\n",
    "                duplicate_cols = [col for col in master_df.columns if col.endswith('_dup')]\n",
    "                master_df = master_df.drop(columns=duplicate_cols)\n",
    "            \n",
    "            merge_info = f\"Inner join merge of {len(successful_dfs)} files\"\n",
    "            \n",
    "        else:  # No merge\n",
    "            # Concatenate all dataframes\n",
    "            master_df = pd.concat(successful_dfs, ignore_index=True)\n",
    "            merge_info = f\"Concatenated {len(successful_dfs)} files without merging\"\n",
    "        \n",
    "        return master_df, merge_info\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None, f\"Merge failed: {str(e)}\"\n",
    "\n",
    "def generate_master_report(master_df, processing_results):\n",
    "    \"\"\"Generate comprehensive report for master dataset\"\"\"\n",
    "    \n",
    "    report_data = {\n",
    "        'Dataset Overview': {\n",
    "            'Total Rows': f\"{len(master_df):,}\",\n",
    "            'Total Columns': len(master_df.columns),\n",
    "            'Unique Job IDs': f\"{master_df['job_id'].nunique():,}\" if 'job_id' in master_df.columns else 'N/A',\n",
    "            'Source Files': master_df['source_file'].nunique() if 'source_file' in master_df.columns else 'N/A'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # File-specific statistics\n",
    "    if 'source_file' in master_df.columns:\n",
    "        file_stats = master_df['source_file'].value_counts()\n",
    "        report_data['Records per Source File'] = file_stats.to_dict()\n",
    "    \n",
    "    # Data quality summary\n",
    "    missing_data = master_df.isnull().sum()\n",
    "    high_missing = missing_data[missing_data > len(master_df) * 0.1]  # >10% missing\n",
    "    \n",
    "    if not high_missing.empty:\n",
    "        report_data['Data Quality Alerts'] = {\n",
    "            'Columns with >10% missing data': high_missing.to_dict()\n",
    "        }\n",
    "    \n",
    "    return report_data\n",
    "\n",
    "# Master dataset creation button\n",
    "create_master_button = widgets.Button(\n",
    "    description='\ud83c\udfaf Create Master Dataset',\n",
    "    button_style='warning',\n",
    "    layout=widgets.Layout(width='220px')\n",
    ")\n",
    "\n",
    "master_output = widgets.Output()\n",
    "\n",
    "def on_create_master_click(b):\n",
    "    with master_output:\n",
    "        clear_output()\n",
    "        \n",
    "        if 'processing_results' not in globals():\n",
    "            display(HTML('<div style=\"background-color: #fff3e0; padding: 10px; border-radius: 5px;\"><strong>\u2139\ufe0f No processed data available.</strong> Please process files first.</div>'))\n",
    "            return\n",
    "        \n",
    "        display(HTML('<div style=\"background-color: #e3f2fd; padding: 10px; border-radius: 5px;\"><strong>\ud83c\udfaf Creating master dataset...</strong></div>'))\n",
    "        \n",
    "        # Create master dataset\n",
    "        master_df, merge_info = create_master_dataset(processing_results, merge_strategy.value)\n",
    "        \n",
    "        if master_df is None:\n",
    "            display(HTML(f'<div style=\"background-color: #ffebee; padding: 10px; border-radius: 5px;\"><strong>\u274c Failed to create master dataset:</strong> {merge_info}</div>'))\n",
    "            return\n",
    "        \n",
    "        # Generate report\n",
    "        report = generate_master_report(master_df, processing_results)\n",
    "        \n",
    "        # Store master dataset globally\n",
    "        global master_dataset\n",
    "        master_dataset = {\n",
    "            'dataframe': master_df,\n",
    "            'report': report,\n",
    "            'merge_info': merge_info\n",
    "        }\n",
    "        \n",
    "        # Display success and preview\n",
    "        display(HTML(f'''\n",
    "        <div style=\"background-color: #e8f5e8; padding: 15px; border-radius: 5px; margin: 10px 0;\">\n",
    "            <strong>\u2705 Master dataset created successfully!</strong><br>\n",
    "            \ud83d\udcca {merge_info}<br>\n",
    "            \ud83d\udcc8 Final dataset: {len(master_df):,} rows \u00d7 {len(master_df.columns)} columns\n",
    "        </div>\n",
    "        '''))\n",
    "        \n",
    "        # Display report\n",
    "        display(HTML('<h4>\ud83d\udccb Master Dataset Report</h4>'))\n",
    "        for section, data in report.items():\n",
    "            display(HTML(f'<h5>{section}</h5>'))\n",
    "            if isinstance(data, dict):\n",
    "                for key, value in data.items():\n",
    "                    display(HTML(f'<strong>{key}:</strong> {value}<br>'))\n",
    "            else:\n",
    "                display(HTML(f'{data}<br>'))\n",
    "        \n",
    "        # Display preview\n",
    "        display(HTML('<h4>\ud83d\udc40 Data Preview (First 5 Rows)</h4>'))\n",
    "        display(master_df.head())\n",
    "        \n",
    "        logger.info(f\"Master dataset created: {len(master_df)} rows, {len(master_df.columns)} columns\")\n",
    "\n",
    "create_master_button.on_click(on_create_master_click)\n",
    "\n",
    "if ENVIRONMENT_READY and ETL_LOGIC_AVAILABLE:\n",
    "    display(create_master_button)\n",
    "    display(master_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfe5187",
   "metadata": {},
   "source": [
    "## 6. Export & Download\n",
    "\n",
    "Save your master dataset and reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb6771f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_master_dataset(master_data, output_name, enable_bq=False, bq_config=None):\n",
    "    \"\"\"Save master dataset to CSV and optionally to BigQuery with improved error handling\"\"\"\n",
    "    \n",
    "    df = master_data['dataframe']\n",
    "    \n",
    "    # Save to CSV\n",
    "    csv_filename = f\"{output_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "    \n",
    "    # Generate report\n",
    "    report_filename = f\"{output_name}_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    master_data['report'].to_csv(report_filename, index=False)\n",
    "    \n",
    "    results = {\n",
    "        'csv_file': csv_filename,\n",
    "        'report_file': report_filename,\n",
    "        'csv_size_mb': os.path.getsize(csv_filename) / (1024 * 1024)\n",
    "    }\n",
    "    \n",
    "    # BigQuery upload if enabled\n",
    "    if enable_bq and BIGQUERY_AVAILABLE and bq_config:\n",
    "        # Validate configuration first\n",
    "        is_valid, validation_message = validate_bigquery_config(bq_config)\n",
    "        if not is_valid:\n",
    "            results['bigquery_success'] = False\n",
    "            results['bigquery_error'] = f\"Configuration error: {validation_message}\"\n",
    "        else:\n",
    "            # Use the improved upload function\n",
    "            upload_result = upload_to_bigquery(df, bq_config)\n",
    "            results['bigquery_success'] = upload_result['success']\n",
    "            \n",
    "            if upload_result['success']:\n",
    "                results['bigquery_table'] = upload_result['table_id']\n",
    "                results['bigquery_rows'] = upload_result['rows_uploaded']\n",
    "            else:\n",
    "                results['bigquery_error'] = upload_result['error']\n",
    "    \n",
    "    return results\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}