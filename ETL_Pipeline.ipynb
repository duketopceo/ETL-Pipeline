{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Robust ETL Cleaning and Merging Pipeline",
        "",
        "**Purpose:** This notebook provides a step-by-step process to upload raw CSV files, clean and standardize the data, merge multiple files into a single dataset, and save the result.",
        "",
        "**Instructions:** Run the cells sequentially from top to bottom. Follow the instructions in the markdown cells for each step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Setup and Configuration",
        "",
        "This cell imports all the necessary Python libraries and the custom cleaning functions from the `etl_pipeline_logic.py` file. It also configures the environment. If this cell runs without errors, your environment is ready."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd",
        "import numpy as np",
        "import logging",
        "import io",
        "import ipywidgets as widgets",
        "from IPython.display import display, HTML",
        "from functools import reduce",
        "import os",
        "",
        "print(\"Importing ETL logic and setting up environment...\")",
        "",
        "if not os.path.exists('etl_pipeline_logic.py'):",
        "    print(\"❌ CRITICAL ERROR: 'etl_pipeline_logic.py' not found.\")",
        "    print(\"Please ensure the logic file is in the same directory as this notebook.\")",
        "    pipeline_ready = False",
        "else:",
        "    try:",
        "        from etl_pipeline_logic import assess_raw_data, clean_csv_data",
        "        print(\"✅ ETL engine imported successfully.\")",
        "        pipeline_ready = True",
        "    except ImportError as e:",
        "        print(f\"❌ Critical import error: {e}. Check the 'etl_pipeline_logic.py' file for issues.\")",
        "        pipeline_ready = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Upload Your Raw Data",
        "",
        "Click the **\"Upload CSVs\"** button below to select the raw data files you want to process. You can select multiple files at once by holding `Ctrl` (or `Cmd` on Mac) and clicking on the files.",
        "",
        "**Important:** Please wait for the upload to complete before moving to the next step. You will see the names of the uploaded files appear next to the button."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "if pipeline_ready:",
        "    uploader = widgets.FileUpload(",
        "        accept='.csv',",
        "        multiple=True,",
        "        description='Upload CSVs',",
        "        button_style='primary'",
        "    )",
        "    display(uploader)",
        "else:",
        "    print(\"Cannot proceed. Pipeline setup failed in the previous step.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Process and Clean Uploaded Data",
        "",
        "This cell performs the core ETL (Extract, Transform, Load) process on each file you uploaded. It will:",
        "1.  **Assess** each file to understand its structure.",
        "2.  **Clean** the data based on the rules defined in `etl_pipeline_logic.py`.",
        "3.  Store the original and cleaned data for reporting and merging.",
        "",
        "You will see log messages indicating the progress for each file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "if 'uploader' in locals() and uploader.value:",
        "    cleaned_dfs = []",
        "    quality_reports = []",
        "    assessments = {}",
        "    ",
        "    print(\"Starting data assessment and cleaning process...\\n\")",
        "",
        "    # The uploader's value is a dictionary-like object. We iterate through its keys.",
        "    for file_index in uploader.value.keys():",
        "        upload = uploader.value[file_index]",
        "        file_name = upload['metadata']['name']",
        "        ",
        "        # --- Error Handling: Check for CSV file extension ---",
        "        if not file_name.lower().endswith('.csv'):",
        "            print(f\"⚠️ Warning: Skipping non-CSV file: '{file_name}'. Only .csv files are supported.\")",
        "            continue",
        "            ",
        "        file_content = upload['content']",
        "",
        "        # 1. Assess Raw Data",
        "        assessment = assess_raw_data(file_content, file_name)",
        "        assessments[file_name] = assessment",
        "        ",
        "        # 2. Clean Data and get the quality report",
        "        cleaned_df, report_df = clean_csv_data(file_content, file_name, assessment)",
        "        ",
        "        if report_df is not None:",
        "            report_df['source_file'] = file_name",
        "            quality_reports.append(report_df)",
        "",
        "        if cleaned_df is not None and 'job_id' in cleaned_df.columns and cleaned_df['job_id'].notna().any():",
        "            cleaned_dfs.append(cleaned_df)",
        "        else:",
        "            print(f\"⚠️ Warning: Could not clean or find job_ids in '{file_name}'. It will be excluded from the merge.\")",
        "    ",
        "    print(\"\\n✅ Processing and cleaning complete.\")",
        "else:",
        "    print(\"❌ Please upload files in Step 2 before running this cell.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Merge Cleaned Data",
        "",
        "This cell merges all the cleaned dataframes into a single, final dataframe. The merge is performed using the `job_id` column, ensuring that data for the same job from different files is combined correctly.",
        "",
        "An **'outer' merge** is used to keep all unique jobs from all files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "final_cleaned_df = None",
        "if 'cleaned_dfs' in locals() and cleaned_dfs:",
        "    if len(cleaned_dfs) == 1:",
        "        final_cleaned_df = cleaned_dfs[0]",
        "        print(\"✅ Only one valid dataframe was cleaned. No merge needed.\")",
        "    else:",
        "        try:",
        "            # Merge all dataframes in the list based on 'job_id'",
        "            final_cleaned_df = reduce(lambda left, right: pd.merge(left, right, on='job_id', how='outer'), cleaned_dfs)",
        "            print(f\"✅ Successfully merged {len(cleaned_dfs)} dataframes.\")",
        "        except Exception as e:",
        "            print(f\"❌ Error during merging: {e}\")",
        "else:",
        "    print(\"❌ No valid dataframes were produced from cleaning. Merge step skipped.\")",
        "",
        "if final_cleaned_df is not None:",
        "    print(f\"Final merged dataset contains {final_cleaned_df.shape[0]} rows and {final_cleaned_df.shape[1]} columns.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: Review Data Quality and Cleaning Report",
        "",
        "This table provides a summary of the cleaning actions performed on every column from the uploaded files. It shows the percentage of missing data before cleaning and the action the script took."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "if 'quality_reports' in locals() and quality_reports:",
        "    # Combine all the individual reports into one",
        "    full_report = pd.concat(quality_reports, ignore_index=True)",
        "    ",
        "    # Reorder columns for better readability",
        "    full_report = full_report[['source_file', 'original_column', 'cleaned_column', 'missing_percent_before', 'action', 'details']]",
        "    ",
        "    # Style the dataframe for better presentation in the notebook",
        "    styled_report = full_report.style.background_gradient(cmap='YlOrRd', subset=['missing_percent_before'])\\
        "        .set_properties(**{'text-align': 'left'})\\
        "        .set_table_styles([dict(selector='th', props=[('text-align', 'left')])])\\
        "        .hide_index()",
        "    ",
        "    print(\"--- Data Quality and Cleaning Report ---\")",
        "    display(styled_report)",
        "else:",
        "    print(\"No data quality reports were generated. Please run Step 3 first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6: Save Cleaned Data to CSV",
        "",
        "This final step saves the cleaned and merged dataframe to a new CSV file named `cleaned_merged_data.csv` in the same directory as this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "if 'final_cleaned_df' in locals() and final_cleaned_df is not None:",
        "    output_filename = 'cleaned_merged_data.csv'",
        "    try:",
        "        final_cleaned_df.to_csv(output_filename, index=False, encoding='utf-8')",
        "        print(f\"\\n✅ Cleaned and merged data saved to '{output_filename}'\")",
        "    except Exception as e:",
        "        print(f\"❌ Error saving file: {e}\")",
        "else:",
        "    print(\"\\n❌ No cleaned and merged data available to save.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 7: Review Detailed Logs",
        "",
        "For a detailed, technical log of the entire cleaning process, you can review the contents of the `etl_cleaning_log.txt` file. The cell below will display its contents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "log_filename = 'etl_cleaning_log.txt'",
        "if os.path.exists(log_filename):",
        "    print(f\"--- Content of {log_filename} ---\\n\")",
        "    with open(log_filename, 'r') as f:",
        "        print(f.read())",
        "    print(f\"--- End of {log_filename} ---\")",
        "else:",
        "    print(f\"Log file '{log_filename}' not found. It will be created when you run the cleaning process.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ee08e1a6d6844edf978d8987d895cc8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FileUploadModel",
          "model_module_version": "1.5.0",
          "state": {
            "_counter": 2,
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FileUploadModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FileUploadView",
            "accept": ".csv",
            "button_style": "",
            "data": [
              null,
              null
            ],
            "description": "Upload CSVs",
            "description_tooltip": null,
            "disabled": false,
            "error": "",
            "icon": "upload",
            "layout": "IPY_MODEL_223cf1c370404a5ea56813d395f226e7",
            "metadata": [
              {
                "name": "slc_data_parttwo_2024.csv",
                "type": "text/csv",
                "size": 491266,
                "lastModified": 1753931644554
              },
              {
                "name": "slc_data_partone_2024.csv",
                "type": "text/csv",
                "size": 244872,
                "lastModified": 1753931581398
              }
            ],
            "multiple": true,
            "style": "IPY_MODEL_2c8d4bff98984cf3bb08e2740680a733"
          }
        },
        "223cf1c370404a5ea56813d395f226e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c8d4bff98984cf3bb08e2740680a733": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}