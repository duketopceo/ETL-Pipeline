{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive and Interactive ETL Pipeline (Corrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a user-friendly interface to a powerful and comprehensive data cleaning engine. \n",
    "**Instructions:** Run the cells in order from top to bottom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Package Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, importlib.util, importlib.metadata\n",
    "from IPython.display import display, HTML\n",
    "print(\"Performing package validation...\")\n",
    "required = {'pandas': '1.3.5', 'numpy': '1.21.5', 'ipywidgets': '7.6.5'}\n",
    "all_ok = True\n",
    "for pkg, ver in required.items():\n",
    "    if importlib.util.find_spec(pkg) is not None:\n",
    "        display(HTML(f\"✅ <b>{pkg}</b>: Installed (Version: {importlib.metadata.version(pkg)}))\"))\n",
    "    else:\n",
    "        all_ok = False\n",
    "        display(HTML(f'❌ <b>{pkg}</b>: Not found. Please install.'))\n",
    "if not all_ok:\n",
    "    display(HTML('<b style=\\\"color:red;\\\">❌ Missing packages. Please install them.</b>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setup Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Importing ETL logic and setting up environment...\")\n",
    "try:\n",
    "    from etl_pipeline_logic import assess_raw_data, clean_csv_data, generate_cleaning_report\n",
    "    import ipywidgets as widgets\n",
    "    import pandas as pd\n",
    "    import io\n",
    "    from functools import reduce\n",
    "    print(\"✅ ETL engine imported successfully.\")\n",
    "    pipeline_ready = True\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Critical import error: {e}. Ensure 'etl_pipeline_logic.py' is in the same directory.\")\n",
    "    pipeline_ready = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Upload Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pipeline_ready:\n",
    "    uploader = widgets.FileUpload(accept='.csv', multiple=True, description='Upload CSVs')\n",
    "    display(uploader)\n",
    "else:\n",
    "    print(\"Cannot proceed. Pipeline setup failed in the previous step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Assess Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'uploader' in locals() and uploader.value:\n",
    "    assessments = {}\n",
    "    for file_upload in uploader.value:\n",
    "        file_name = file_upload['name']\n",
    "        file_content = file_upload['content']\n",
    "        assessments[file_name] = assess_raw_data(file_content, file_name)\n",
    "    print(\"\\nAssessment phase complete. See 'etl_cleaning_log.txt' for detailed logs.\")\n",
    "else:\n",
    "    print(\"Please upload files in the previous step before assessing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Clean and Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'uploader' in locals() and uploader.value:\n",
    "    cleaned_dfs = []\n",
    "    original_dfs = {}\n",
    "    print(\"Starting data cleaning process...\")\n",
    "    \n",
    "    for file_upload in uploader.value:\n",
    "        file_name = file_upload['name']\n",
    "        file_content = file_upload['content']\n",
    "        \n",
    "        original_dfs[file_name] = pd.read_csv(io.BytesIO(file_content), on_bad_lines='skip')\n",
    "        \n",
    "        assessment = assessments.get(file_name)\n",
    "        cleaned_df = clean_csv_data(file_content, file_name, assessment)\n",
    "        \n",
    "        if cleaned_df is not None and 'job_id' in cleaned_df.columns and cleaned_df['job_id'].notna().any():\n",
    "            cleaned_dfs.append(cleaned_df)\n",
    "        else:\n",
    "            print(f\"⚠️ Warning: Could not clean or find job_ids in '{file_name}'. It will be excluded from the merge.\")\n",
    "\n",
    "    final_cleaned_df = None\n",
    "    if not cleaned_dfs:\n",
    "        print(\"❌ No valid dataframes with job_ids were produced. Merge step skipped.\")\n",
    "    elif len(cleaned_dfs) == 1:\n",
    "        final_cleaned_df = cleaned_dfs[0]\n",
    "        print(\"✅ Only one valid dataframe. No merge needed.\")\n",
    "    else:\n",
    "        try:\n",
    "            final_cleaned_df = reduce(lambda left, right: pd.merge(left, right, on='job_id', how='outer'), cleaned_dfs)\n",
    "            print(f\"✅ Successfully merged {len(cleaned_dfs)} dataframes into a final dataset with shape {final_cleaned_df.shape}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error during merging: {e}\")\n",
    "else:\n",
    "    print(\"Please upload files first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Generate Final Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'final_cleaned_df' in locals() and final_cleaned_df is not None:\n",
    "    # For the final report, we can't easily tie back to original_dfs after a merge.\n",
    "    # We will generate a report on the final merged dataframe itself.\n",
    "    print(\"\\n--- FINAL REPORT ON MERGED DATA ---\")\n",
    "    report = generate_cleaning_report(final_cleaned_df, final_cleaned_df, 'merged_dataset') # Simplified report\n",
    "    print(f\"  Final Rows: {report['cleaned_dataset']['rows']}\")\n",
    "    print(f\"  Final Columns: {report['cleaned_dataset']['columns']}\")\n",
    "    print(f\"  Memory Usage: {report['cleaned_dataset']['memory_usage_mb']:.2f} MB\")\n",
    "    print(\"\\nSee 'etl_cleaning_log.txt' for full details.\")\n",
    "else:\n",
    "    print(\"No cleaned data available to report on.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
