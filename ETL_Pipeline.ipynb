{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RoofLink Data ETL Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome! This notebook is a tool to help you clean and process raw data exported from RoofLink. It will guide you through uploading your CSV files, cleaning them, merging them into a single dataset, and exporting the result.\\n",
    "\\n",
    "**Instructions:**\\n",
    "1. Click on a code cell (the boxes with `[ ]:` next to them).\\n",
    "2. Press **Shift + Enter** to run the cell.\\n",
    "3. Run the cells in order from top to bottom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Check Required Packages\\n",
    "This first step checks to make sure all the necessary software packages are installed. If you see a green checkmark for each package, you're good to go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, importlib.util, importlib.metadata\\n",
    "from IPython.display import display, HTML\\n",
    "print(\"Performing package validation...\")\\n",
    "required = {'pandas': '1.3.5', 'numpy': '1.21.5', 'ipywidgets': '7.6.5'}\\n",
    "all_ok = True\\n",
    "for pkg, ver in required.items():\\n",
    "    if importlib.util.find_spec(pkg) is not None:\\n",
    "        display(HTML(f\"✅ <b>{pkg}</b>: Installed (Version: {importlib.metadata.version(pkg)}))\"))\\n",
    "    else:\\n",
    "        all_ok = False\\n",
    "        display(HTML(f'❌ <b>{pkg}</b>: Not found. Please install.'))\\n",
    "if not all_ok:\\n",
    "    display(HTML('<b style=\\\"color:red;\\\">❌ Missing packages. Please install them.</b>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Set Up the Pipeline Engine\\n",
    "This cell imports the main data cleaning logic from the `etl_pipeline_logic.py` script. You should see a success message below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Importing ETL logic and setting up environment...\")\\n",
    "import sys\\n",
    "# Ensure the current directory is in the system path so the notebook can find the logic script.\\n",
    "if '.' not in sys.path:\\n",
    "    sys.path.append('.')\\n",
    "\\n",
    "try:\\n",
    "    # Now, we can import our custom logic.\\n",
    "    from etl_pipeline_logic import assess_raw_data, clean_csv_data, generate_cleaning_report\\n",
    "    import ipywidgets as widgets\\n",
    "    import pandas as pd\\n",
    "    import io\\n",
    "    from functools import reduce\\n",
    "    print(\"✅ ETL engine imported successfully.\")\\n",
    "    pipeline_ready = True\\n",
    "except ImportError as e:\\n",
    "    print(f\"❌ Critical import error: {e}. Ensure 'etl_pipeline_logic.py' is in the same directory.\")\\n",
    "    pipeline_ready = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Upload Your CSV Files\\n",
    "Click the 'Upload CSVs' button below to select one or more of your data files. You can select multiple files at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pipeline_ready:\\n",
    "    uploader = widgets.FileUpload(accept='.csv', multiple=True, description='Upload CSVs')\\n",
    "    display(uploader)\\n",
    "else:\\n",
    "    print(\"Cannot proceed. Pipeline setup failed in the previous step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Assess Raw Data\\n",
    "This step performs a quick check on your uploaded files to understand their structure before we start cleaning. The results are used in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'uploader' in locals() and uploader.value:\\n",
    "    assessments = {}\\n",
    "    print(\"Assessing uploaded files...\")\\n",
    "    for file_upload in uploader.value:\\n",
    "        file_name = file_upload['name']\\n",
    "        file_content = file_upload['content']\\n",
    "        assessments[file_name] = assess_raw_data(file_content, file_name)\\n",
    "    print(\"\\n✅ Assessment phase complete. You can see detailed logs in 'etl_cleaning_log.txt'.\")\\n",
    "else:\\n",
    "    print(\"Please upload files in the previous step before assessing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Clean and Merge Data\\n",
    "This is the main event! The pipeline will now clean each of your files, then merge them together into a single dataset using the `job_id`.\\n",
    "\\n",
    "**Pay close attention to the output of this cell.** It will print detailed logs about the cleaning process, including warnings about missing data or other issues it finds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'uploader' in locals() and uploader.value:\\n",
    "    cleaned_dfs = []\n",
    "    \n",
    "    print(\"Starting data cleaning and merging process...\")\n",
    "    \n",
    "    for file_upload in uploader.value:\n",
    "        file_name = file_upload['name']\n",
    "        file_content = file_upload['content']\n",
    "        \n",
    "        assessment = assessments.get(file_name)\n",
    "        cleaned_df = clean_csv_data(file_content, file_name, assessment)\n",
    "        \n",
    "        if cleaned_df is not None and 'job_id' in cleaned_df.columns and cleaned_df['job_id'].notna().any():\n",
    "            cleaned_dfs.append(cleaned_df)\n",
    "        else:\n",
    "            print(f\"⚠️ Warning: Could not clean or find job_ids in '{file_name}'. It will be excluded from the merge.\")\n",
    "\n",
    "    final_cleaned_df = None\n",
    "    if not cleaned_dfs:\n",
    "        print(\"❌ No valid dataframes with job_ids were produced. Merge step skipped.\")\n",
    "    elif len(cleaned_dfs) == 1:\n",
    "        final_cleaned_df = cleaned_dfs[0]\n",
    "        print(\"\\n✅ Only one valid dataframe. No merge needed.\")\n",
    "    else:\n",
    "        print(f\"\\nAttempting to merge {len(cleaned_dfs)} cleaned dataframes...\")\n",
    "        try:\n",
    "            # Set job_id as the index for all dataframes before combining.\n",
    "            indexed_dfs = [df.set_index('job_id') for df in cleaned_dfs if 'job_id' in df.columns]\n",
    "            \n",
    "            # Use combine_first to intelligently merge, filling NaNs from one dataframe with data from the next.\n",
    "            final_merged = indexed_dfs[0]\n",
    "            for i in range(1, len(indexed_dfs)):\n",
    "                final_merged = final_merged.combine_first(indexed_dfs[i])\n",
    "            \n",
    "            final_cleaned_df = final_merged.reset_index()\n",
    "            print(f\"✅ Successfully merged dataframes into a final dataset with shape {final_cleaned_df.shape}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error during merging: {e}\")\n",
    "else:\n",
    "    print(\"Please upload files first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Display Final Data Preview\\n",
    "Let's take a look at the first few rows of the final, cleaned, and merged dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'final_cleaned_df' in locals() and final_cleaned_df is not None:\\n",
    "    print(\"--- PREVIEW OF FINAL MERGED DATA ---\")\\n",
    "    display(final_cleaned_df.head())\\n",
    "else:\\n",
    "    print(\"No cleaned data available to display.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Export Cleaned Data to CSV\\n",
    "This final step will save your cleaned and merged data into a new file named `cleaned_rooflink_data.csv` in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'final_cleaned_df' in locals() and final_cleaned_df is not None:\\n",
    "    output_filename = 'cleaned_rooflink_data.csv'\\n",
    "    try:\\n",
    "        final_cleaned_df.to_csv(output_filename, index=False)\\n",
    "        print(f\"✅ Success! Your cleaned data has been saved as '{output_filename}'.\")\\n",
    "    except Exception as e:\\n",
    "        print(f\"❌ Error saving file: {e}\")\\n",
    "else:\\n",
    "    print(\"No cleaned data available to export.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
