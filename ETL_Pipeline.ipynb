{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXrEfaOd7jAb"
      },
      "source": [
        "# Comprehensive and Interactive ETL Pipeline (Corrected)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbDH14Af7jAe"
      },
      "source": [
        "This notebook provides a user-friendly interface to a powerful and comprehensive data cleaning engine.\n",
        "**Instructions:** Run the cells in order from top to bottom."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIs53TkR7jAf"
      },
      "source": [
        "### 1. Package Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "XZ5Ze5oc7jAg",
        "outputId": "67e01409-d1f7-4b36-9785-d5cf08620d4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing package validation...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "✅ <b>pandas</b>: Installed (Version: 2.2.2))"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "✅ <b>numpy</b>: Installed (Version: 2.0.2))"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "✅ <b>ipywidgets</b>: Installed (Version: 7.7.1))"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import sys, importlib.util, importlib.metadata\n",
        "from IPython.display import display, HTML\n",
        "print(\"Performing package validation...\")\n",
        "required = {'pandas': '1.3.5', 'numpy': '1.21.5', 'ipywidgets': '7.6.5'}\n",
        "all_ok = True\n",
        "for pkg, ver in required.items():\n",
        "    if importlib.util.find_spec(pkg) is not None:\n",
        "        display(HTML(f\"✅ <b>{pkg}</b>: Installed (Version: {importlib.metadata.version(pkg)}))\"))\n",
        "    else:\n",
        "        all_ok = False\n",
        "        display(HTML(f'❌ <b>{pkg}</b>: Not found. Please install.'))\n",
        "if not all_ok:\n",
        "    display(HTML('<b style=\\\"color:red;\\\">❌ Missing packages. Please install them.</b>'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nk3CWoEN7jAk"
      },
      "source": [
        "### 2. Setup Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZj8JxxM7jAk",
        "outputId": "f5b6db0d-ecf2-4f5d-89db-acf227d7d451"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Importing ETL logic and setting up environment...\n",
            "✅ ETL engine imported successfully.\n"
          ]
        }
      ],
      "source": [
        "print(\"Importing ETL logic and setting up environment...\")\n",
        "try:\n",
        "    from etl_pipeline_logic import assess_raw_data, clean_csv_data, generate_cleaning_report\n",
        "    import ipywidgets as widgets\n",
        "    import pandas as pd\n",
        "    import io\n",
        "    from functools import reduce\n",
        "    print(\"✅ ETL engine imported successfully.\")\n",
        "    pipeline_ready = True\n",
        "except ImportError as e:\n",
        "    print(f\"❌ Critical import error: {e}. Ensure 'etl_pipeline_logic.py' is in the same directory.\")\n",
        "    pipeline_ready = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVHcU6347jAm"
      },
      "source": [
        "### 3. Upload Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "ee08e1a6d6844edf978d8987d895cc8c",
            "223cf1c370404a5ea56813d395f226e7",
            "2c8d4bff98984cf3bb08e2740680a733"
          ]
        },
        "id": "SS_2PJcJ7jAm",
        "outputId": "b572c94a-df60-47ec-c0d3-afa6557dc76f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "FileUpload(value={}, accept='.csv', description='Upload CSVs', multiple=True)"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee08e1a6d6844edf978d8987d895cc8c"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "if pipeline_ready:\n",
        "    uploader = widgets.FileUpload(accept='.csv', multiple=True, description='Upload CSVs')\n",
        "    display(uploader)\n",
        "else:\n",
        "    print(\"Cannot proceed. Pipeline setup failed in the previous step.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkYK0nmq7jAn"
      },
      "source": [
        "### 4. Assess Raw Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63qV-_AR7jAn",
        "outputId": "f34b59b1-2cae-4a9b-be50-53f218659f51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-08-28 18:28:19,791 - INFO - ASSESSING: slc_data_parttwo_2024.csv\n",
            "2025-08-28 18:28:19,806 - INFO - ASSESSING: slc_data_partone_2024.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Assessment phase complete. See 'etl_cleaning_log.txt' for detailed logs.\n",
            "\n",
            "--- Content of etl_cleaning_log.txt ---\n",
            "2025-08-28 18:27:48,354 - INFO - ASSESSING: slc_data_parttwo_2024.csv\n",
            "2025-08-28 18:27:48,382 - INFO - ASSESSING: slc_data_partone_2024.csv\n",
            "2025-08-28 18:28:19,791 - INFO - ASSESSING: slc_data_parttwo_2024.csv\n",
            "2025-08-28 18:28:19,806 - INFO - ASSESSING: slc_data_partone_2024.csv\n",
            "\n",
            "--- End of etl_cleaning_log.txt ---\n"
          ]
        }
      ],
      "source": [
        "if 'uploader' in locals() and uploader.value:\n",
        "    assessments = {}\n",
        "    # Iterate over the keys (which are integers) to access the uploaded files\n",
        "    for file_index in uploader.value.keys():\n",
        "        file_upload = uploader.value[file_index]\n",
        "        file_name = file_upload['metadata']['name']\n",
        "        file_content = file_upload['content']\n",
        "        assessments[file_name] = assess_raw_data(file_content, file_name)\n",
        "    print(\"\\nAssessment phase complete. See 'etl_cleaning_log.txt' for detailed logs.\")\n",
        "\n",
        "    # Read and print the log file content\n",
        "    try:\n",
        "        with open('etl_cleaning_log.txt', 'r') as f:\n",
        "            log_content = f.read()\n",
        "            print(\"\\n--- Content of etl_cleaning_log.txt ---\")\n",
        "            print(log_content)\n",
        "            print(\"--- End of etl_cleaning_log.txt ---\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"\\nError: etl_cleaning_log.txt not found.\")\n",
        "\n",
        "else:\n",
        "    print(\"Please upload files in the previous step before assessing.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwkJAnGJ7jAn"
      },
      "source": [
        "### 5. Clean and Merge Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gg45B10O7jAo",
        "outputId": "bd6397f1-c843-4915-e2a0-b768657e158c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-08-28 18:28:50,641 - INFO - ============================================================\n",
            "2025-08-28 18:28:50,644 - INFO - STARTING DATA CLEANING FOR: slc_data_parttwo_2024.csv\n",
            "2025-08-28 18:28:50,645 - INFO - ============================================================\n",
            "2025-08-28 18:28:50,677 - INFO - Loaded 1213 raw rows.\n",
            "2025-08-28 18:28:50,739 - INFO - Removing 3 likely summary/header rows from data.\n",
            "2025-08-28 18:28:50,753 - INFO - Found URL column: 'full_url'. Extracting job_id.\n",
            "2025-08-28 18:28:50,773 - INFO - Successfully extracted 1210 job IDs.\n",
            "2025-08-28 18:28:50,778 - WARNING - Dropped column 'estimate' due to >90% missing values.\n",
            "2025-08-28 18:28:50,806 - WARNING - Dropped column 'date_approved' due to >90% missing values.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting data cleaning process...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-08-28 18:28:50,812 - WARNING - Dropped column 'date_signed' due to >90% missing values.\n",
            "2025-08-28 18:28:50,815 - WARNING - Dropped column 'date_rd_released' due to >90% missing values.\n",
            "2025-08-28 18:28:50,820 - WARNING - Dropped column 'date_completed' due to >90% missing values.\n",
            "2025-08-28 18:28:50,825 - WARNING - Dropped column 'date_roof_completed' due to >90% missing values.\n",
            "2025-08-28 18:28:50,828 - WARNING - Dropped column 'roofing_crew' due to >90% missing values.\n",
            "2025-08-28 18:28:50,835 - WARNING - Dropped column 'date_closed' due to >90% missing values.\n",
            "2025-08-28 18:28:50,838 - WARNING - Dropped column 'insurance_company_name' due to >90% missing values.\n",
            "2025-08-28 18:28:50,841 - WARNING - Dropped column 'insurance_company_claims_phone' due to >90% missing values.\n",
            "2025-08-28 18:28:50,843 - WARNING - Dropped column 'insurance_company_claims_extension' due to >90% missing values.\n",
            "2025-08-28 18:28:50,846 - WARNING - Dropped column 'insurance_company_claims_email' due to >90% missing values.\n",
            "2025-08-28 18:28:50,848 - WARNING - Dropped column 'claim_handler_name' due to >90% missing values.\n",
            "2025-08-28 18:28:50,851 - WARNING - Dropped column 'claim_handler_phone' due to >90% missing values.\n",
            "2025-08-28 18:28:50,853 - WARNING - Dropped column 'claim_handler_email' due to >90% missing values.\n",
            "2025-08-28 18:28:51,007 - INFO - CLEANING COMPLETE for slc_data_parttwo_2024.csv. Final shape: (1210, 28)\n",
            "2025-08-28 18:28:51,029 - INFO - ============================================================\n",
            "2025-08-28 18:28:51,031 - INFO - STARTING DATA CLEANING FOR: slc_data_partone_2024.csv\n",
            "2025-08-28 18:28:51,033 - INFO - ============================================================\n",
            "2025-08-28 18:28:51,056 - INFO - Loaded 632 raw rows.\n",
            "2025-08-28 18:28:51,095 - INFO - Found URL column: 'full_url'. Extracting job_id.\n",
            "2025-08-28 18:28:51,100 - INFO - Successfully extracted 632 job IDs.\n",
            "2025-08-28 18:28:51,103 - WARNING - Dropped column 'estimate' due to >90% missing values.\n",
            "2025-08-28 18:28:51,105 - WARNING - Dropped column 'estimate_date_last_edited' due to >90% missing values.\n",
            "2025-08-28 18:28:51,107 - WARNING - Dropped column 'estimate_gt_margin' due to >90% missing values.\n",
            "2025-08-28 18:28:51,111 - WARNING - Dropped column 'estimate_estimate_gross_margin' due to >90% missing values.\n",
            "2025-08-28 18:28:51,113 - WARNING - Dropped column 'estimate_owes' due to >90% missing values.\n",
            "2025-08-28 18:28:51,116 - WARNING - Dropped column 'estimate_total' due to >90% missing values.\n",
            "2025-08-28 18:28:51,135 - WARNING - Dropped column 'customer_marketing_rep' due to >90% missing values.\n",
            "2025-08-28 18:28:51,145 - WARNING - Dropped column 'date_approved' due to >90% missing values.\n",
            "2025-08-28 18:28:51,148 - WARNING - Dropped column 'date_signed' due to >90% missing values.\n",
            "2025-08-28 18:28:51,151 - WARNING - Dropped column 'date_rd_released' due to >90% missing values.\n",
            "2025-08-28 18:28:51,154 - WARNING - Dropped column 'date_completed' due to >90% missing values.\n",
            "2025-08-28 18:28:51,157 - WARNING - Dropped column 'date_roof_completed' due to >90% missing values.\n",
            "2025-08-28 18:28:51,159 - WARNING - Dropped column 'roofing_crew' due to >90% missing values.\n",
            "2025-08-28 18:28:51,163 - WARNING - Dropped column 'date_closed' due to >90% missing values.\n",
            "2025-08-28 18:28:51,166 - WARNING - Dropped column 'insurance_company_name' due to >90% missing values.\n",
            "2025-08-28 18:28:51,168 - WARNING - Dropped column 'insurance_company_claims_phone' due to >90% missing values.\n",
            "2025-08-28 18:28:51,170 - WARNING - Dropped column 'insurance_company_claims_extension' due to >90% missing values.\n",
            "2025-08-28 18:28:51,172 - WARNING - Dropped column 'insurance_company_claims_email' due to >90% missing values.\n",
            "2025-08-28 18:28:51,174 - WARNING - Dropped column 'claim_handler_name' due to >90% missing values.\n",
            "2025-08-28 18:28:51,176 - WARNING - Dropped column 'claim_handler_phone' due to >90% missing values.\n",
            "2025-08-28 18:28:51,178 - WARNING - Dropped column 'claim_handler_email' due to >90% missing values.\n",
            "2025-08-28 18:28:51,254 - INFO - CLEANING COMPLETE for slc_data_partone_2024.csv. Final shape: (632, 22)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Successfully merged 2 dataframes into a final dataset with shape (1842, 49).\n"
          ]
        }
      ],
      "source": [
        "if 'uploader' in locals() and uploader.value:\n",
        "    cleaned_dfs = []\n",
        "    original_dfs = {}\n",
        "    print(\"Starting data cleaning process...\")\n",
        "\n",
        "    for file_index in uploader.value.keys():\n",
        "        file_upload = uploader.value[file_index]\n",
        "        file_name = file_upload['metadata']['name']\n",
        "        file_content = file_upload['content']\n",
        "\n",
        "        original_dfs[file_name] = pd.read_csv(io.BytesIO(file_content), on_bad_lines='skip')\n",
        "\n",
        "        assessment = assessments.get(file_name)\n",
        "        cleaned_df = clean_csv_data(file_content, file_name, assessment)\n",
        "\n",
        "        if cleaned_df is not None and 'job_id' in cleaned_df.columns and cleaned_df['job_id'].notna().any():\n",
        "            cleaned_dfs.append(cleaned_df)\n",
        "        else:\n",
        "            print(f\"⚠️ Warning: Could not clean or find job_ids in '{file_name}'. It will be excluded from the merge.\")\n",
        "\n",
        "    final_cleaned_df = None\n",
        "    if not cleaned_dfs:\n",
        "        print(\"❌ No valid dataframes with job_ids were produced. Merge step skipped.\")\n",
        "    elif len(cleaned_dfs) == 1:\n",
        "        final_cleaned_df = cleaned_dfs[0]\n",
        "        print(\"✅ Only one valid dataframe. No merge needed.\")\n",
        "    else:\n",
        "        try:\n",
        "            final_cleaned_df = reduce(lambda left, right: pd.merge(left, right, on='job_id', how='outer'), cleaned_dfs)\n",
        "            print(f\"✅ Successfully merged {len(cleaned_dfs)} dataframes into a final dataset with shape {final_cleaned_df.shape}.\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error during merging: {e}\")\n",
        "else:\n",
        "    print(\"Please upload files first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxg4K2c57jAo"
      },
      "source": [
        "### 6. Generate Final Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGdjW2zD7jAo",
        "outputId": "c73a0ed2-adf3-4532-87dc-30464e8f6437"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-08-28 18:29:37,949 - INFO - GENERATING REPORT FOR: merged_dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- FINAL REPORT ON MERGED DATA ---\n",
            "  Final Rows: 1842\n",
            "  Final Columns: 49\n",
            "  Rows Removed During Cleaning: 0\n",
            "\n",
            "See 'etl_cleaning_log.txt' for full details.\n"
          ]
        }
      ],
      "source": [
        "if 'final_cleaned_df' in locals() and final_cleaned_df is not None:\n",
        "    # For the final report, we can't easily tie back to original_dfs after a merge.\n",
        "    # We will generate a report on the final merged dataframe itself.\n",
        "    print(\"\\n--- FINAL REPORT ON MERGED DATA ---\")\n",
        "    report = generate_cleaning_report(final_cleaned_df, final_cleaned_df, 'merged_dataset') # Simplified report\n",
        "    print(f\"  Final Rows: {report['cleaned_dataset']['rows']}\")\n",
        "    print(f\"  Final Columns: {report['cleaned_dataset']['columns']}\")\n",
        "    # Check if 'improvements' key exists before accessing it\n",
        "    if 'improvements' in report and 'rows_removed' in report['improvements']:\n",
        "        print(f\"  Rows Removed During Cleaning: {report['improvements']['rows_removed']}\")\n",
        "    print(\"\\nSee 'etl_cleaning_log.txt' for full details.\")\n",
        "else:\n",
        "    print(\"No cleaned data available to report on.\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "919a3ce7",
        "outputId": "5bdb9d78-0070-4efa-98dd-cf9ebe3dbdc1"
      },
      "source": [
        "if 'final_cleaned_df' in locals() and final_cleaned_df is not None:\n",
        "    output_filename = 'cleaned_merged_data.csv'\n",
        "    final_cleaned_df.to_csv(output_filename, index=False)\n",
        "    print(f\"\\n✅ Cleaned and merged data saved to '{output_filename}'\")\n",
        "else:\n",
        "    print(\"\\n❌ No cleaned and merged data available to save.\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Cleaned and merged data saved to 'cleaned_merged_data.csv'\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ee08e1a6d6844edf978d8987d895cc8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FileUploadModel",
          "model_module_version": "1.5.0",
          "state": {
            "_counter": 2,
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FileUploadModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FileUploadView",
            "accept": ".csv",
            "button_style": "",
            "data": [
              null,
              null
            ],
            "description": "Upload CSVs",
            "description_tooltip": null,
            "disabled": false,
            "error": "",
            "icon": "upload",
            "layout": "IPY_MODEL_223cf1c370404a5ea56813d395f226e7",
            "metadata": [
              {
                "name": "slc_data_parttwo_2024.csv",
                "type": "text/csv",
                "size": 491266,
                "lastModified": 1753931644554
              },
              {
                "name": "slc_data_partone_2024.csv",
                "type": "text/csv",
                "size": 244872,
                "lastModified": 1753931581398
              }
            ],
            "multiple": true,
            "style": "IPY_MODEL_2c8d4bff98984cf3bb08e2740680a733"
          }
        },
        "223cf1c370404a5ea56813d395f226e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c8d4bff98984cf3bb08e2740680a733": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}