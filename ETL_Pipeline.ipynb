{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Pipeline for Job Data Consolidation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements an interactive ETL pipeline. You can upload multiple CSV files containing job data, and the notebook will automatically extract a `job_id` from a URL column, merge the files, and perform data completeness checks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Upload CSV Files\n",
    "\n",
    "Use the button below to select and upload one or more CSV files from your computer. After uploading, proceed to the next step to process the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "uploader = widgets.FileUpload(\n",
    "    accept='.csv',\n",
    "    multiple=True,\n",
    "    description='Upload CSVs'\n",
    ")\n",
    "\n",
    "display(uploader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Process and Load Uploaded Data\n",
    "\n",
    "This step reads the content of the uploaded files into pandas DataFrames. It will display the first few rows of each loaded DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "if not uploader.value:\n",
    "    print(\"Please upload at least one CSV file in the step above.\")\n",
    "else:\n",
    "    dfs = []\n",
    "    for file_info in uploader.value:\n",
    "        content = file_info['content']\n",
    "        df = pd.read_csv(io.BytesIO(content))\n",
    "        dfs.append(df)\n",
    "        print(f\"Loaded {file_info['name']}:\")\n",
    "        display(df.head())\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract Job ID\n",
    "\n",
    "Next, we extract a numeric `job_id` from a URL column in each DataFrame. The code will attempt to automatically find the URL column by looking for 'url' or 'link' in the column name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def find_url_column(df):\n",
    "    for col in df.columns:\n",
    "        if 'url' in col.lower() or 'link' in col.lower():\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "def extract_job_id(df, url_column):\n",
    "    if url_column is None:\n",
    "        print(\"Warning: Could not find a URL column. Skipping job ID extraction.\")\n",
    "        df['job_id'] = None\n",
    "        return df\n",
    "    \n",
    "    df['job_id'] = df[url_column].str.extract(r'(\\d+)', expand=False)\n",
    "    df['job_id'] = pd.to_numeric(df['job_id'], errors='coerce')\n",
    "    return df\n",
    "\n",
    "processed_dfs = []\n",
    "for i, df in enumerate(dfs):\n",
    "    url_col = find_url_column(df)\n",
    "    df = extract_job_id(df.copy(), url_col)\n",
    "    \n",
    "    # Assertion for missing job_ids\n",
    "    missing_ids = df['job_id'].isnull().sum()\n",
    "    if missing_ids > 0:\n",
    "        print(f\"Warning: Found {missing_ids} rows in DataFrame {i} with a missing job_id after extraction.\")\n",
    "    \n",
    "    # Drop the original URL column if it was found\n",
    "    if url_col:\n",
    "        df = df.drop(columns=[url_col])\n",
    "        \n",
    "    processed_dfs.append(df)\n",
    "    print(f\"Processed DataFrame {i} with job_id:\")\n",
    "    display(df)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Merge Datasets\n",
    "\n",
    "All datasets are now merged into a single DataFrame using the `job_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "if not processed_dfs:\n",
    "    print(\"No data to merge.\")\n",
    "else:\n",
    "    # Merge all dataframes in the list\n",
    "    merged_df = reduce(lambda left, right: pd.merge(left, right, on='job_id', how='outer'), processed_dfs)\n",
    "    \n",
    "    # Assertion for merge failure\n",
    "    assert len(merged_df) >= max(len(df) for df in processed_dfs), \"Merge failed unexpectedly.\"\n",
    "\n",
    "    print(\"Merged Dataset:\")\n",
    "    display(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Completeness Testing\n",
    "\n",
    "Finally, we perform completeness tests on the merged data, checking for null values and empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def check_completeness(df):\n",
    "    df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "    \n",
    "    missing_values = df.isnull().sum()\n",
    "    total_rows = len(df)\n",
    "    completeness_percentage = ((total_rows - missing_values) / total_rows) * 100\n",
    "    \n",
    "    completeness_summary = pd.DataFrame({\n",
    "        'missing_values': missing_values,\n",
    "        'completeness_percentage': completeness_percentage\n",
    "    })\n",
    "    \n",
    "    # Warning for columns with >5% missing values\n",
    "    for index, row in completeness_summary.iterrows():\n",
    "        if (100 - row['completeness_percentage']) > 5:\n",
    "            print(f\"Warning: Column '{index}' has more than 5% missing values ({100 - row['completeness_percentage']:.2f}%).\")\n",
    "            \n",
    "    return completeness_summary\n",
    "\n",
    "if 'merged_df' in locals():\n",
    "    completeness_report = check_completeness(merged_df)\n",
    "    print(\"Data Completeness Report:\")\n",
    "    display(completeness_report)\n",
    "    print(\"\\n--- Markdown Summary ---\")\n",
    "    print(completeness_report.to_markdown())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
